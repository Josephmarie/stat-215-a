This file is your grade for the final project. Your grade is the sum of your
individual grade (based on collaboration) and the group grade.
Penalties for tardiness and other inconsistencies will be applied separately.
First you will see your total grade, then your individual grade, then your group grade.

- Anthony

TOTAL GRADE (0-90):
49.0
================================================================================
Below is your final project individual grade penalty.
It is in the form of a penalty to your individual grade, only
applied if I learned there were substantial issues with your
collaboration (or your group as a whole). Most students had
a penalty of zero.

================================================================================
group:
g3
===================================PENALTIES====================================
Total penalty:
0 (mean: -0.67, median: 0)
penalty for not submitting review {-2, 0}:
0 (mean: -0.095, median: 0)
collaboration penalty (-∞, 0]:
0 (mean: -0.57, median: 0)
====================================REVIEWS=====================================
self-rating:
5 (mean: 4.5, median: 5)
rating from reviewer 1:
3
rating from reviewer 2:
3
rating from reviewer 3:
4
================================================================================
Below is your final project group grade.
The highest possible score on the final project is 90 points.
Pay attention to your total score relative to other groups, rather than your absolute score out of 90.
Each numerical score lists the mean, median, and maximum score among all groups.

Note: Groups who analyzed non-TBI data were given the aforementioned +6% bonus (so 5.4 points).

I recommend reading the feedback carefully.
Feel free to ask privately (or publicly) on Ed to discuss your grade.

================================================================================
TOTAL (0-90):
49 (mean: 67.4, median: 71, max: 85.4, sd: 12.3)
===================================SUBSCORES====================================
SKELETON AND REPORT (0-6):
5 (mean: 5.8, median: 6, max: 6)
CODING (0-5):
2 (mean: 2.8, median: 3, max: 4)
REPRODUCIBILITY (0-5):
2 (mean: 2.8, median: 2, max: 4)
DATA (0-13):
9 (mean: 10.4, median: 12, max: 13)
METRIC (0-6):
2 (mean: 3.8, median: 4, max: 5)
MODELING (0-16):
8 (mean: 11.6, median: 11, max: 16)
MODEL COMPARISON (0-10):
7 (mean: 8, median: 8, max: 9)
MODEL INTERPRETATION (0-6):
5 (mean: 3.4, median: 4, max: 5)
STABILITY (0-8):
2 (mean: 6.4, median: 8, max: 8)
FINAL ASSESSMENT (0-10):
5 (mean: 7, median: 6, max: 9)
FIGURES (0-5):
2 (mean: 3.2, median: 3, max: 5)
BONUS FOR NON-TBI:
0 (mean: 2.16, median: 0, max: 5.4)
====================================SKELETON====================================
run.sh:
1 (mean: 1, median: 1, max: 1)
environment.yaml:
1 (mean: 1, median: 1, max: 1)
final-project.tex (need this or ipynb):
1 (mean: 1, median: 1, max: 1)
final-project.ipynb (need this or tex):
0 (mean: 0, median: 0, max: 0)
collaboration.txt:
1 (mean: 1, median: 1, max: 1)
=====================================REPORT=====================================
≤ 15pg:
0 (mean: 0.8, median: 1, max: 1)
No code:
1 (mean: 1, median: 1, max: 1)
===================================CODE STYLE===================================
comments (0-2):
1 (mean: 1.2, median: 1, max: 2)
docstrings (0-2):
1 (mean: 1.4, median: 1, max: 2)
DNNs use config files (0-1):
0 (mean: 0.2, median: 0, max: 1)
comments:
  Some parts of code are completely undocumented, like "CustomImageDataset".
  Many functions missing docstrings.
================================REPRODUCIBILITY=================================
Clear what does what (0-2):
2 (mean: 1.8, median: 2, max: 2)
DNN checkpoints provided (0-1):
0 (mean: 0.4, median: 0, max: 1)
environment.yaml works and is sufficient (0-1):
0 (mean: 0.2, median: 0, max: 1)
Code passes static checking (0-1):
0 (mean: 0.4, median: 0, max: 1)
comments:
  Great idea to explain what each file does in run.sh. Your environment.yaml is
  missing imbalanced-learn, torchvision, tensorflow, and scipy. Undefined
  variables test_df and model in logreg.ipynb
======================================DATA======================================
Data collection description (0-2):
1 (mean: 1.4, median: 2, max: 2)
Quality of data cleaning (1-4):
2 (mean: 2.8, median: 3, max: 4)
Level of detail / transparency of data cleaning (1-3):
3 (mean: 2.8, median: 3, max: 3)
comments:
  Good effort at describing data collection, just need more detail. When was the
  study run? Who collected the data? How did they do follow-up to measure the
  ciTBI outcome? Good effort at data cleaning, you just missed a few things:
  should "other" really be NaN, as it seems distinct from a missing value? Is
  "Not Applicable" really the same thing as a missing value (NaN)? (e.g.
  consider cases where AMS is False, then AMSSleep will be Not Applicable, but
  it seems you know what AMSSleep is in that case (False, as AMS would be true
  otherwise), so it should be imputed to False; under your strategy you will get
  mostly NaN in AMSSleep and will remove the column due to high missingness).
  There were also many variables you could validate using domain knowledge
  (though you did validate GCS). Your description is excellent and I think I
  could recreate your exact data cleaning process just using what you wrote.
Discussion of data splitting (0-4):
3 (mean: 3.4, median: 3, max: 4)
comments:
  Good idea to oversample for training data (although in your models the same
  effect can be accomplished by appropriate loss functions or weighting of
  samples). Would have been ideal to discuss generalization in the more
  important sense (unseen data from the future at a new hospital)
=====================================METRIC=====================================
choice of metric(s) (0-3):
1 (mean: 2, median: 2, max: 3)
discussion/justification of metric(s) (0-3):
1 (mean: 1.8, median: 2, max: 2)
comments:
  FNR is easy to hack here by a model just outputting "Positive" for everything.
  You don't consider that doctors want to also not have a lot of false
  positives, as CT has a risk of cancer.
====================================MODELING====================================
Implemented baseline (0-2):
0 (mean: 1.4, median: 2, max: 2)
comments:
  I don't see any implementation of the Kuppermann et al model; this is
  essential as a baseline to compare to
Description of non-DL model(s) (0-4):
2 (mean: 3, median: 3, max: 4)
Narrative of development process (0-2):
1 (mean: 1.4, median: 1, max: 2)
comments:
  Yeah the descriptions here are definitely written by chatgpt. You have a lot
  of words but they don't really say anything. For RF I am confused on how both
  oversampling and class weighting are being used, and you don't state any
  important hyperparameters other than number of trees. For LDA and QDA you
  don't address any concerns about applying them to binary predictors. Scaling
  variables in logistic regression doesn't change the predictions unless you are
  regularizing (which isn't standard in logistic regression and you only mention
  on the side), so your description there (also clearly written by chatgpt)
  doesn't make sense.
Description of DL model(s) (0-4):
2 (mean: 3.4, median: 4, max: 4)
DL model has no obvious issues hurting performance (0-2):
2 (mean: 1, median: 1, max: 2)
Narrative of development process (0-2):
1 (mean: 1.4, median: 1, max: 2)
comments:
  Great job defining custom loss functions for the neural networks to emphasize
  false negatives. You need more description of how the models were designed and
  why. And what were their final architectures? For the CNN: why would tabular
  data have spatial relationships making a CNN appropriate? This needs to be
  justified, and chatgpt does not do a good job. CNNs are most appropriate when,
  across parts of a grid, there are repeateable relationships between nearby
  cells (e.g. pixels in an image); this doesn't hold so nicely for tabular data
  reshaped into an arbitrary grid shape.
======================MODEL COMPARISON AND INTERPRETATION=======================
Detail in model performance evaluation (0-3):
3 (mean: 3, median: 3, max: 3)
Evaluations consistent across models (0-1):
1 (mean: 1, median: 1, max: 1)
comments:
  Great!
Detail and justification in model selection (0-3):
2 (mean: 2.2, median: 2, max: 3)
Use dataset not used in model training/validation, or address concerns (0-1):
1 (mean: 0.6, median: 1, max: 1)
Interpretability considered in selection (0-1):
0 (mean: 0.4, median: 0, max: 1)
Baseline included in evaluation/selection (0-1):
0 (mean: 0.8, median: 1, max: 1)
comments:
  Since your choice of metric is flawed as described above, your evaluation here
  is flawed too. You also did not consider interpretability or the performance
  of the baseline.
Detail of model interpretation (0-4):
3 (mean: 3, median: 3, max: 4)
Interpret the model in words, focusing on directionality (e.g. "having high ___ leads to a Positive prediction") (0-2):
2 (mean: 0.4, median: 0, max: 2)
comments:
  The analysis in section 8 (written by chatgpt) is okay for generally comparing
  the interpretability of CNNs and logistic regression. Section 12 is good; it
  would have been better to integrate that into section 8. Excellent idea to
  look at the direction of the coefficients in 12.2, you were the only group
  that took this key step! But your analysis is uncritical (e.g., why is
  AgeinYear positive while AgeInMonth is negative? Does that really make sense?)
===================================STABILITY====================================
model perturbation (0-2):
0 (mean: 1.6, median: 2, max: 2)
data perturbation: covariate shift (0-2):
0 (mean: 1.6, median: 2, max: 2)
data perturbation: subgroup (0-2):
0 (mean: 1.4, median: 2, max: 2)
data perturbation: other (0-2):
2 (mean: 1.8, median: 2, max: 2)
================================FINAL ASSESSMENT================================
final evaluation depth (0-4):
2 (mean: 2.8, median: 2, max: 4)
evaluate on data not used in model selection, or address concerns (0-1):
0 (mean: 0.4, median: 0, max: 1)
mention or address uncertainty in evaluation metrics (0-1):
0 (mean: 0, median: 0, max: 0)
comments:
  You do have an evaluation of the model when you compare models, but to
  complete the narrative of the report you really need to present a final
  evaluation like "On a final, held-out dataset, the CNN and logistic regression
  have __% and __% sensitivity, which is better/worse than the baseline..." etc.
discussion of the real world, trust in the model, etc (0-4):
3 (mean: 3.8, median: 4, max: 4)
comments:
  Clearly written by chatgpt, but covers most of the important points. Would be
  ideal to consider how and when the model might actually be used by a doctor
====================================FIGURES=====================================
Overall figure quality (0-5):
2 (mean: 3.2, median: 3, max: 5)
comments:
  Text is too small in most figures. Figure 3 diverging colormap should be
  centered at 0. Figure 6 doesn’t really tell us anything. Default matplotlib in
  Fig 2; and Fig 2 doesn’t really give any benefit over a small table.
