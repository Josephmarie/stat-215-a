{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d180e66d-9858-4e71-84df-7edc20f6d25a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frodo\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ddc2c3f-1a15-4a7e-a2b1-0c43414bb74a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: memory-profiler in ./.local/lib/python3.12/site-packages (0.61.0)\n",
      "Requirement already satisfied: psutil in /usr/local/linux/miniforge-3.12/lib/python3.12/site-packages (from memory-profiler) (5.9.8)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /usr/local/linux/miniforge-3.12/lib/python3.12/site-packages (4.66.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install memory-profiler\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "988ffe07-d1a8-4e91-91f4-04a63c3c1f10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lingBinary.csv...\n",
      "Parameters: kmax=10, B=100, m=0.8\n",
      "Loading sampled data...\n",
      "Performing MiniBatch k-means clustering with 100 batches...\n",
      "Processing similarities in 37 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [02:51<00:00,  4.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Metrics:\n",
      "Execution time: 172.62 seconds\n",
      "Peak memory usage: 5007.20 MB\n",
      "Matrix shape: (36121, 36121)\n",
      "\n",
      "Similarity Matrix Statistics:\n",
      "Min similarity: 0.000\n",
      "Max similarity: 1.000\n",
      "Mean similarity: 0.211\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import jit, prange\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "\n",
    "@jit(nopython=True, parallel=True, fastmath=True)\n",
    "def compute_cpu_similarity(features1, features2, cluster_weights1, cluster_weights2):\n",
    "    n, m = features1.shape[0], features2.shape[0]\n",
    "    result = np.empty((n, m), dtype=np.float32)\n",
    "\n",
    "    for i in prange(n):\n",
    "        f1 = features1[i]\n",
    "        c1 = cluster_weights1[i]\n",
    "        for j in prange(m):\n",
    "            f2 = features2[j]\n",
    "            c2 = cluster_weights2[j]\n",
    "            \n",
    "            # Let's compute similarities with minimal memory usage\n",
    "            intersection_orig = 0.0\n",
    "            union_orig = 0.0\n",
    "            for k in range(len(f1)):\n",
    "                min_val = min(f1[k], f2[k])\n",
    "                max_val = max(f1[k], f2[k])\n",
    "                intersection_orig += min_val\n",
    "                union_orig += max_val\n",
    "            \n",
    "            orig_sim = intersection_orig / union_orig if union_orig > 0 else 0.0\n",
    "\n",
    "            intersection_clust = 0.0\n",
    "            union_clust = 0.0\n",
    "            for k in range(len(c1)):\n",
    "                min_val = min(c1[k], c2[k])\n",
    "                max_val = max(c1[k], c2[k])\n",
    "                intersection_clust += min_val\n",
    "                union_clust += max_val\n",
    "            \n",
    "            clust_sim = intersection_clust / union_clust if union_clust > 0 else 0.0\n",
    "            result[i, j] = 0.5 * (orig_sim + clust_sim)\n",
    "\n",
    "    return result\n",
    "\n",
    "class MemoryEfficientSimilarityComputer:\n",
    "    def __init__(self, max_chunk_size=100, B=100):\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        self.B = B  \n",
    "\n",
    "    def estimate_memory_usage(self, n_samples, n_features):\n",
    "        bytes_per_float = 4 \n",
    "        estimated_memory = (n_samples * n_features * bytes_per_float * 3) / (1024 ** 2) \n",
    "        return estimated_memory\n",
    "\n",
    "    def get_optimal_chunk_size(self, n_samples, n_features):\n",
    "        available_memory = psutil.virtual_memory().available / (1024 ** 2)\n",
    "        memory_per_sample = self.estimate_memory_usage(1, n_features)\n",
    "        optimal_chunk_size = int(min(\n",
    "            self.max_chunk_size,\n",
    "            (available_memory * 0.5) / memory_per_sample  \n",
    "        ))\n",
    "        return max(1, optimal_chunk_size)\n",
    "\n",
    "    def compute_similarity_matrix(self, data_iterator, total_samples, n_features, m=0.8, k=3):\n",
    "        q = int(total_samples * m)\n",
    "        \n",
    "        # Let's calculate batch size based on B parameter\n",
    "        batch_size = max(1, q // self.B)\n",
    "        \n",
    "        # Sample indices\n",
    "        indices = np.random.choice(total_samples, q, replace=False)\n",
    "        indices.sort()  \n",
    "        \n",
    "        # Let's initialize storage for sampled data\n",
    "        subsampled_data = np.zeros((q, n_features), dtype=np.float32)\n",
    "        \n",
    "        # Let's load sampled data\n",
    "        print(\"Loading sampled data...\")\n",
    "        current_idx = 0\n",
    "        sample_idx = 0\n",
    "        for chunk in data_iterator:\n",
    "            chunk_array = chunk.to_numpy(dtype=np.float32)\n",
    "            chunk_size = len(chunk_array)\n",
    "            chunk_end = current_idx + chunk_size\n",
    "            \n",
    "            # Let's find indices that fall within this chunk\n",
    "            while sample_idx < len(indices) and indices[sample_idx] < chunk_end:\n",
    "                relative_idx = indices[sample_idx] - current_idx\n",
    "                if relative_idx >= 0:\n",
    "                    subsampled_data[sample_idx] = chunk_array[relative_idx]\n",
    "                sample_idx += 1\n",
    "            \n",
    "            current_idx = chunk_end\n",
    "            if sample_idx >= len(indices):\n",
    "                break\n",
    "        \n",
    "        # Let's clear memory\n",
    "        gc.collect()\n",
    "        \n",
    "        # Let's perform clustering with B batches\n",
    "        print(f\"Performing MiniBatch k-means clustering with {self.B} batches...\")\n",
    "        kmeans = MiniBatchKMeans(\n",
    "            n_clusters=k,\n",
    "            batch_size=batch_size,\n",
    "            n_init='auto',\n",
    "            random_state=42,\n",
    "            max_iter=self.B\n",
    "        )\n",
    "        cluster_labels = kmeans.fit_predict(subsampled_data)\n",
    "        distances = kmeans.transform(subsampled_data)\n",
    "        \n",
    "        # Let's compute cluster weights\n",
    "        cluster_weights = np.exp(-distances / 0.1)\n",
    "        cluster_weights /= cluster_weights.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        # Let's clear memory\n",
    "        del distances\n",
    "        gc.collect()\n",
    "        \n",
    "        # Let's initialize result matrix\n",
    "        similarity_matrix = np.zeros((q, q), dtype=np.float32)\n",
    "        \n",
    "        # Let's compute similarities in chunks\n",
    "        chunk_size = self.get_optimal_chunk_size(q, n_features)\n",
    "        n_chunks = (q + chunk_size - 1) // chunk_size\n",
    "        print(f\"Processing similarities in {n_chunks} chunks...\")\n",
    "        \n",
    "        for i in tqdm(range(0, q, chunk_size)):\n",
    "            end_idx = min(i + chunk_size, q)\n",
    "            chunk_data = subsampled_data[i:end_idx]\n",
    "            chunk_weights = cluster_weights[i:end_idx]\n",
    "            \n",
    "            similarity_matrix[i:end_idx] = compute_cpu_similarity(\n",
    "                chunk_data,\n",
    "                subsampled_data,\n",
    "                chunk_weights,\n",
    "                cluster_weights\n",
    "            )\n",
    "            \n",
    "            # Let's clear memory after each chunk\n",
    "            gc.collect()\n",
    "        \n",
    "        return similarity_matrix\n",
    "\n",
    "def get_binary_csv_iterator(file_path, chunk_size=1000):\n",
    "    \"\"\"Create iterator for binary-only columns from CSV.\"\"\"\n",
    "    # Load a small sample to detect binary columns\n",
    "    df_info = pd.read_csv(file_path, nrows=100)\n",
    "    binary_columns = df_info.columns[(df_info.isin([0, 1]).all())]\n",
    "    \n",
    "    return pd.read_csv(\n",
    "        file_path,\n",
    "        usecols=binary_columns,\n",
    "        chunksize=chunk_size,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "def compute_similarity_from_csv(csv_file, kmax=3, B=100, m=0.2):\n",
    "    print(f\"Processing {csv_file}...\")\n",
    "    print(f\"Parameters: kmax={kmax}, B={B}, m={m}\")\n",
    "    \n",
    "    file_size = os.path.getsize(csv_file) / (1024 ** 2)\n",
    "    chunk_size = min(1000, max(100, int(50 * 1024 / file_size)))\n",
    "    \n",
    "    csv_iterator = get_binary_csv_iterator(csv_file, chunk_size)\n",
    "    \n",
    "    total_rows = sum(1 for _ in open(csv_file)) - 1\n",
    "    first_chunk = next(csv_iterator)\n",
    "    n_features = first_chunk.shape[1]\n",
    "    \n",
    "    csv_iterator = get_binary_csv_iterator(csv_file, chunk_size)\n",
    "    \n",
    "    computer = MemoryEfficientSimilarityComputer(max_chunk_size=chunk_size, B=B)\n",
    "    \n",
    "    start_memory = psutil.Process().memory_info().rss / (1024 ** 2)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    similarity_matrix = computer.compute_similarity_matrix(\n",
    "        csv_iterator,\n",
    "        total_rows,\n",
    "        n_features,\n",
    "        m=m,\n",
    "        k=kmax\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    end_memory = psutil.Process().memory_info().rss / (1024 ** 2)\n",
    "    \n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(f\"Execution time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Peak memory usage: {end_memory - start_memory:.2f} MB\")\n",
    "    print(f\"Matrix shape: {similarity_matrix.shape}\")\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file_path = \"lingBinary.csv\"\n",
    "    \n",
    "    try:\n",
    "        similarity_matrix = compute_similarity_from_csv(\n",
    "            csv_file_path,\n",
    "            kmax=10,\n",
    "            B=100,\n",
    "            m=0.8\n",
    "        )\n",
    "        \n",
    "        print(\"\\nSimilarity Matrix Statistics:\")\n",
    "        print(f\"Min similarity: {similarity_matrix.min():.3f}\")\n",
    "        print(f\"Max similarity: {similarity_matrix.max():.3f}\")\n",
    "        print(f\"Mean similarity: {similarity_matrix.mean():.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac73a2f6-b81e-4700-92b7-438180a90044",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of the Similarity Matrix (first 5x5):\n",
      "[[1.         0.16053256 0.22335605 0.17087835 0.14970274]\n",
      " [0.16053256 1.         0.43323073 0.49403    0.3304244 ]\n",
      " [0.22335605 0.43323073 1.         0.39416608 0.2961134 ]\n",
      " [0.17087835 0.49403    0.39416608 1.         0.19232179]\n",
      " [0.14970274 0.3304244  0.2961134  0.19232179 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print a sample of the similarity matrix (first 5x5 block)\n",
    "print(\"\\nSample of the Similarity Matrix (first 5x5):\")\n",
    "print(similarity_matrix[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460c3e7-7421-4278-928d-f29be4f4913a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def save_similarity_matrix_to_csv(matrix, filename=\"similarity_matrix.csv\"):\n",
    "    df = pd.DataFrame(matrix)\n",
    "    df.to_csv(filename, index=True)\n",
    "    print(f\"Similarity matrix saved to '{filename}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    similarity_matrix = similarity_matrix\n",
    "\n",
    "    # Save the matrix to a CSV file\n",
    "    save_similarity_matrix_to_csv(similarity_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (ipykernel)",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
